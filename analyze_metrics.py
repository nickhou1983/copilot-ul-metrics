#!/usr/bin/env python3
"""
GitHub Copilot Metrics - æ•°æ®åˆ†æç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç”Ÿæˆçš„ CSV æ–‡ä»¶è¿›è¡Œæ•°æ®åˆ†æ
"""

import pandas as pd
import glob
from pathlib import Path


def analyze_user_summary(csv_file: str):
    """åˆ†æç”¨æˆ·æ€»ä½“æŒ‡æ ‡"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ç”¨æˆ·æ€»ä½“æŒ‡æ ‡åˆ†æ")
    print("=" * 70)
    
    df = pd.read_csv(csv_file)
    
    print(f"\nğŸ“ˆ åŸºç¡€ç»Ÿè®¡:")
    print(f"   æ€»ç”¨æˆ·æ•°: {df['user_id'].nunique()}")
    print(f"   æ€»è®°å½•æ•°: {len(df)}")
    print(f"   æ•°æ®æ—¥æœŸèŒƒå›´: {df['day'].min()} è‡³ {df['day'].max()}")
    
    print(f"\nğŸ¯ æ´»åŠ¨æŒ‡æ ‡:")
    print(f"   æ€»äº¤äº’æ¬¡æ•°: {df['user_initiated_interaction_count'].sum():,}")
    print(f"   æ€»ä»£ç ç”Ÿæˆæ¬¡æ•°: {df['code_generation_activity_count'].sum():,}")
    print(f"   æ€»ä»£ç æ¥å—æ¬¡æ•°: {df['code_acceptance_activity_count'].sum():,}")
    print(f"   å¹³å‡æ¥å—ç‡: {df['acceptance_rate'].mean():.2f}%")
    
    print(f"\nğŸ“ ä»£ç è¡Œæ•°ç»Ÿè®¡:")
    print(f"   æ€»å»ºè®®æ–°å¢è¡Œæ•°: {df['loc_suggested_to_add_sum'].sum():,}")
    print(f"   æ€»å®é™…æ–°å¢è¡Œæ•°: {df['loc_added_sum'].sum():,}")
    print(f"   æ€»å®é™…åˆ é™¤è¡Œæ•°: {df['loc_deleted_sum'].sum():,}")
    print(f"   å¹³å‡é‡‡çº³ç‡: {df['adoption_rate'].mean():.2f}%")
    
    print(f"\nğŸš€ é«˜çº§åŠŸèƒ½é‡‡ç”¨:")
    agent_users = df['used_agent'].sum()
    chat_users = df['used_chat'].sum()
    total_users = df['user_id'].nunique()
    print(f"   ä½¿ç”¨ Agent çš„è®°å½•æ•°: {agent_users} ({agent_users/len(df)*100:.1f}%)")
    print(f"   ä½¿ç”¨ Chat çš„è®°å½•æ•°: {chat_users} ({chat_users/len(df)*100:.1f}%)")
    
    print(f"\nğŸ† TOP 10 æœ€æ´»è·ƒç”¨æˆ· (æŒ‰ä»£ç ç”Ÿæˆæ¬¡æ•°):")
    top_users = df.groupby('user_login').agg({
        'code_generation_activity_count': 'sum',
        'code_acceptance_activity_count': 'sum',
        'loc_added_sum': 'sum'
    }).sort_values('code_generation_activity_count', ascending=False).head(10)
    
    for idx, (user, row) in enumerate(top_users.iterrows(), 1):
        print(f"   {idx:2d}. {user:30s} - ç”Ÿæˆ: {row['code_generation_activity_count']:4.0f}, "
              f"æ¥å—: {row['code_acceptance_activity_count']:4.0f}, "
              f"æ–°å¢è¡Œæ•°: {row['loc_added_sum']:5.0f}")


def analyze_by_feature(csv_file: str):
    """åˆ†æåŠŸèƒ½ç»´åº¦"""
    print("\n" + "=" * 70)
    print("âš¡ åŠŸèƒ½ç»´åº¦åˆ†æ")
    print("=" * 70)
    
    df = pd.read_csv(csv_file)
    
    # æŒ‰åŠŸèƒ½èšåˆ
    feature_stats = df.groupby('feature').agg({
        'code_generation_activity_count': 'sum',
        'code_acceptance_activity_count': 'sum',
        'loc_suggested_to_add_sum': 'sum',
        'loc_added_sum': 'sum'
    }).sort_values('code_generation_activity_count', ascending=False)
    
    print(f"\nğŸ“Š å„åŠŸèƒ½ä½¿ç”¨ç»Ÿè®¡:")
    for feature, row in feature_stats.iterrows():
        acceptance_rate = (row['code_acceptance_activity_count'] / row['code_generation_activity_count'] * 100) if row['code_generation_activity_count'] > 0 else 0
        print(f"\n   ã€{feature}ã€‘")
        print(f"      ä»£ç ç”Ÿæˆæ¬¡æ•°: {row['code_generation_activity_count']:,.0f}")
        print(f"      ä»£ç æ¥å—æ¬¡æ•°: {row['code_acceptance_activity_count']:,.0f}")
        print(f"      æ¥å—ç‡: {acceptance_rate:.2f}%")
        print(f"      å»ºè®®æ–°å¢è¡Œæ•°: {row['loc_suggested_to_add_sum']:,.0f}")
        print(f"      å®é™…æ–°å¢è¡Œæ•°: {row['loc_added_sum']:,.0f}")


def analyze_by_language(csv_file: str):
    """åˆ†æç¼–ç¨‹è¯­è¨€ç»´åº¦"""
    print("\n" + "=" * 70)
    print("ğŸ”¤ ç¼–ç¨‹è¯­è¨€ç»´åº¦åˆ†æ")
    print("=" * 70)
    
    df = pd.read_csv(csv_file)
    
    # æŒ‰è¯­è¨€èšåˆ
    lang_stats = df.groupby('language').agg({
        'code_generation_activity_count': 'sum',
        'code_acceptance_activity_count': 'sum',
        'loc_suggested_to_add_sum': 'sum',
        'loc_added_sum': 'sum'
    }).sort_values('code_generation_activity_count', ascending=False).head(10)
    
    print(f"\nğŸ“Š TOP 10 ä½¿ç”¨æœ€å¤šçš„ç¼–ç¨‹è¯­è¨€:")
    for idx, (lang, row) in enumerate(lang_stats.iterrows(), 1):
        acceptance_rate = (row['code_acceptance_activity_count'] / row['code_generation_activity_count'] * 100) if row['code_generation_activity_count'] > 0 else 0
        print(f"\n   {idx:2d}. ã€{lang}ã€‘")
        print(f"       ä»£ç ç”Ÿæˆæ¬¡æ•°: {row['code_generation_activity_count']:,.0f}")
        print(f"       ä»£ç æ¥å—æ¬¡æ•°: {row['code_acceptance_activity_count']:,.0f}")
        print(f"       æ¥å—ç‡: {acceptance_rate:.2f}%")
        print(f"       å®é™…æ–°å¢è¡Œæ•°: {row['loc_added_sum']:,.0f}")


def analyze_by_ide(csv_file: str):
    """åˆ†æIDEç»´åº¦"""
    print("\n" + "=" * 70)
    print("ğŸ› ï¸ IDE ç»´åº¦åˆ†æ")
    print("=" * 70)
    
    df = pd.read_csv(csv_file)
    
    # æŒ‰IDEèšåˆ
    ide_stats = df.groupby('ide').agg({
        'user_initiated_interaction_count': 'sum',
        'code_generation_activity_count': 'sum',
        'code_acceptance_activity_count': 'sum',
        'loc_added_sum': 'sum'
    }).sort_values('code_generation_activity_count', ascending=False)
    
    print(f"\nğŸ“Š å„ IDE ä½¿ç”¨ç»Ÿè®¡:")
    for ide, row in ide_stats.iterrows():
        acceptance_rate = (row['code_acceptance_activity_count'] / row['code_generation_activity_count'] * 100) if row['code_generation_activity_count'] > 0 else 0
        print(f"\n   ã€{ide.upper()}ã€‘")
        print(f"      ç”¨æˆ·äº¤äº’æ¬¡æ•°: {row['user_initiated_interaction_count']:,.0f}")
        print(f"      ä»£ç ç”Ÿæˆæ¬¡æ•°: {row['code_generation_activity_count']:,.0f}")
        print(f"      ä»£ç æ¥å—æ¬¡æ•°: {row['code_acceptance_activity_count']:,.0f}")
        print(f"      æ¥å—ç‡: {acceptance_rate:.2f}%")
        print(f"      å®é™…æ–°å¢è¡Œæ•°: {row['loc_added_sum']:,.0f}")


def analyze_by_model(csv_file: str):
    """åˆ†æAIæ¨¡å‹ç»´åº¦"""
    print("\n" + "=" * 70)
    print("ğŸ¤– AI æ¨¡å‹ç»´åº¦åˆ†æ")
    print("=" * 70)
    
    df = pd.read_csv(csv_file)
    
    # æŒ‰æ¨¡å‹èšåˆ
    model_stats = df.groupby('model').agg({
        'user_initiated_interaction_count': 'sum',
        'code_generation_activity_count': 'sum',
        'code_acceptance_activity_count': 'sum',
        'loc_suggested_to_add_sum': 'sum',
        'loc_added_sum': 'sum'
    }).sort_values('code_generation_activity_count', ascending=False)
    
    print(f"\nğŸ“Š å„ AI æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡:")
    for model, row in model_stats.iterrows():
        acceptance_rate = (row['code_acceptance_activity_count'] / row['code_generation_activity_count'] * 100) if row['code_generation_activity_count'] > 0 else 0
        print(f"\n   ã€{model}ã€‘")
        print(f"      ç”¨æˆ·äº¤äº’æ¬¡æ•°: {row['user_initiated_interaction_count']:,.0f}")
        print(f"      ä»£ç ç”Ÿæˆæ¬¡æ•°: {row['code_generation_activity_count']:,.0f}")
        print(f"      ä»£ç æ¥å—æ¬¡æ•°: {row['code_acceptance_activity_count']:,.0f}")
        print(f"      æ¥å—ç‡: {acceptance_rate:.2f}%")
        print(f"      å»ºè®®æ–°å¢è¡Œæ•°: {row['loc_suggested_to_add_sum']:,.0f}")
        print(f"      å®é™…æ–°å¢è¡Œæ•°: {row['loc_added_sum']:,.0f}")


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸ¯ GitHub Copilot User Level Metrics - æ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 70)
    
    # æŸ¥æ‰¾æ‰€æœ‰ CSV æ–‡ä»¶
    csv_files = glob.glob("*_*.csv")
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ° CSV æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ json_to_csv.py ç”Ÿæˆ CSV æ–‡ä»¶")
        return
    
    print(f"\næ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶")
    
    # åˆ†æç”¨æˆ·æ€»ä½“æŒ‡æ ‡
    user_summary_files = [f for f in csv_files if '_user_summary.csv' in f]
    if user_summary_files:
        analyze_user_summary(user_summary_files[0])
    
    # åˆ†æåŠŸèƒ½ç»´åº¦
    feature_files = [f for f in csv_files if '_by_feature.csv' in f]
    if feature_files:
        analyze_by_feature(feature_files[0])
    
    # åˆ†æç¼–ç¨‹è¯­è¨€ç»´åº¦
    lang_feature_files = [f for f in csv_files if '_by_language_feature.csv' in f]
    if lang_feature_files:
        analyze_by_language(lang_feature_files[0])
    
    # åˆ†æIDEç»´åº¦
    ide_files = [f for f in csv_files if '_by_ide.csv' in f]
    if ide_files:
        analyze_by_ide(ide_files[0])
    
    # åˆ†æAIæ¨¡å‹ç»´åº¦
    model_feature_files = [f for f in csv_files if '_by_model_feature.csv' in f]
    if model_feature_files:
        analyze_by_model(model_feature_files[0])
    
    print("\n" + "=" * 70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 70 + "\n")


if __name__ == '__main__':
    main()
